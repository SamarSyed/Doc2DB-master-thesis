{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a small working example of the first Doc2DB system version.\n",
    "Please run all cells below the headline \"Run all cells below first\" first.\n",
    "Then choose a one of the sample texts or any text file you want to try.\n",
    "Run the cell with the Doc2DB processing, display the resulting dataframe if you like and then filter it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text = read_file() # choose a txt file from the file dialogue\n",
    "\n",
    "raw_text = 'Thorfinn Rowle was a Dark wizard and a Death Eater who fought in the Second Wizarding War.\\nHe fought at the Battle of the Astronomy Tower, where he accidentally killed fellow Death Eater Gibbon, burned Rubeus Hagrid\\'s cabin, and tortured Harry Potter.\\nOn 1 August, 1997, he tracked Harry Potter, Ron Weasley, and Hermione Granger to Tottenham Court Road along with Antonin Dolohov disguised as construction workers, but the two Death Eaters were defeated and their memories were modified by Hermione to cover their escape.\\nAfter this failure, Rowle was punished severely by Lord Voldemort.\\nHe also fought at the Battle of Hogwarts on 2 May, 1998.\\nRowle\\'s fate after the Second Wizarding War and Voldemort\\'s final defeat is uncertain.\\nRowle was one of the Death Eaters of Lord Voldemort who fought in the Second Wizarding War.\\nIt is unknown if he had also fought in the First Wizarding War, though the fact that Ronald Weasley recognised him as well as Antonin Dolohov from old wanted posters suggests that he was. Harry joined Ron for the next Battle. He fought against Antonin and saved Ron. Harry was a really good magician.'\n",
    "# raw_text = \"'' Ron '' Weasley * * ( b . 1 March , 1980 ) was an English pure-blood [ 2 ] wizard , the sixth and youngest son of Arthur and Molly Weasley ( n√©e Prewett ) . He was also the younger brother of Bill , Charlie , Percy , Fred , George , and the elder brother of Ginny . Ron and his siblings lived at the The Burrow , on the outskirts of Ottery St Catchpole , Devon . Ron began attending Hogwarts School of Witchcraft and Wizardry in 1991 and was Sorted into Gryffindor House . He soon became close friends with fellow student Harry Potter and later Hermione Granger . \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing batch corpus...\n",
      "Batch corpus prepared. Number of paragraphs =  1\n",
      "Time taken for preperation:  0:00:00.008999\n",
      "resolving sentences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203c39a70d3343eb8ccec09d595a3a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samar Syed\\anaconda3\\lib\\site-packages\\torch\\autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences resolved. Number of sentences =  1\n",
      "Time taken for resolving:  0:00:05.535882\n",
      "extracting triples\n",
      "extracting triples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b66effe7ceb4c4baa00d82f91b4d2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting server with command: java -Xmx8G -cp C:\\Users\\Samar Syed\\.stanfordnlp_resources\\stanford-corenlp-4.1.0/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 60000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-9d958e165d884cbe.props -preload openie\n",
      "Time taken for triple extraction:  0:00:11.106011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samar Syed\\anaconda3\\lib\\site-packages\\thefuzz\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1183e01cb64afca5b8eabf7044f44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc7c117ab02442f89cade1f9443a6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters = 9\n",
      "Classifying the points into clusters:\n",
      "[5 5 4 2 1 4 6 0 7 4 3 7 4 6 3 1 0 4 2 0 6 1 6 6 8]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecb5090bd0a423ba4c36fdfef7eee71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for processing:  0:00:20.360632\n"
     ]
    }
   ],
   "source": [
    "# Doc2DB processing\n",
    "\n",
    "start_time = time.time()\n",
    "test2 = text_processor_pipeline(raw_text)\n",
    "df, cluster, relation_list_set = prepare_df(test2, raw_text)\n",
    "vectors, d = prepare_for_filtering(cluster, relation_list_set)\n",
    "save_df(df, 'testing')\n",
    "end_time = time.time()\n",
    "print('Time taken for processing: ', str(timedelta(seconds=end_time-start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_cluster_number</th>\n",
       "      <th>object</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{be}</td>\n",
       "      <td>4</td>\n",
       "      <td>Dark</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{be}</td>\n",
       "      <td>4</td>\n",
       "      <td>Dark wizard</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the First Wizarding War</td>\n",
       "      <td>EVENT</td>\n",
       "      <td>{after, be, in}</td>\n",
       "      <td>1</td>\n",
       "      <td>the Second Wizarding War</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{be}</td>\n",
       "      <td>4</td>\n",
       "      <td>wizard</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{torture}</td>\n",
       "      <td>5</td>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{burn}</td>\n",
       "      <td>0</td>\n",
       "      <td>Rubeus Hagrid  cabin</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{kill, accidentally}</td>\n",
       "      <td>0</td>\n",
       "      <td>fellow Death Eater Gibbon</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{kill, accidentally}</td>\n",
       "      <td>0</td>\n",
       "      <td>Death Eater Gibbon</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{fight, at}</td>\n",
       "      <td>6</td>\n",
       "      <td>Battle of Astronomy Tower</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{also, fight, at}</td>\n",
       "      <td>6</td>\n",
       "      <td>the Battle of the Astronomy Tower</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Rubeus Hagrid</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{have}</td>\n",
       "      <td>4</td>\n",
       "      <td>cabin</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>two Death Eaters</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>{be, by, defeat}</td>\n",
       "      <td>0</td>\n",
       "      <td>Hermione Granger</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{on, track, at_time}</td>\n",
       "      <td>1</td>\n",
       "      <td>1 August, 1997</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hermione Granger</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{be, along}</td>\n",
       "      <td>1</td>\n",
       "      <td>Antonin Dolohov</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Harry Potter</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{as, disguise}</td>\n",
       "      <td>3</td>\n",
       "      <td>construction workers</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hermione Granger</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{as, disguise}</td>\n",
       "      <td>3</td>\n",
       "      <td>construction workers</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hermione Granger</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{be, to}</td>\n",
       "      <td>4</td>\n",
       "      <td>Tottenham Court Road</td>\n",
       "      <td>FAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ron Weasley</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{as, disguise}</td>\n",
       "      <td>3</td>\n",
       "      <td>construction workers</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{be, punish, severely, by}</td>\n",
       "      <td>5</td>\n",
       "      <td>Lord Voldemort</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{also, fight, at}</td>\n",
       "      <td>6</td>\n",
       "      <td>the Battle of Hogwarts</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{also, on, fight}</td>\n",
       "      <td>6</td>\n",
       "      <td>2 May 1998</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>final defeat</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>{be, uncertain, 's}</td>\n",
       "      <td>7</td>\n",
       "      <td>Voldemort</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Rowle  fate</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>{after, be, uncertain}</td>\n",
       "      <td>7</td>\n",
       "      <td>the Second Wizarding War</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>defeat</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>{after, be, uncertain}</td>\n",
       "      <td>7</td>\n",
       "      <td>the Second Wizarding War</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>final defeat</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>{after, be, uncertain}</td>\n",
       "      <td>7</td>\n",
       "      <td>the Second Wizarding War</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    subject subject_type                    relation  \\\n",
       "0            Thorfinn Rowle       PERSON                        {be}   \n",
       "1            Thorfinn Rowle       PERSON                        {be}   \n",
       "2   the First Wizarding War        EVENT             {after, be, in}   \n",
       "3            Thorfinn Rowle       PERSON                        {be}   \n",
       "4            Thorfinn Rowle       PERSON                   {torture}   \n",
       "5            Thorfinn Rowle       PERSON                      {burn}   \n",
       "6            Thorfinn Rowle       PERSON        {kill, accidentally}   \n",
       "7            Thorfinn Rowle       PERSON        {kill, accidentally}   \n",
       "8            Thorfinn Rowle       PERSON                 {fight, at}   \n",
       "9            Thorfinn Rowle       PERSON           {also, fight, at}   \n",
       "10            Rubeus Hagrid       PERSON                      {have}   \n",
       "11         two Death Eaters      Unknown            {be, by, defeat}   \n",
       "12           Thorfinn Rowle       PERSON        {on, track, at_time}   \n",
       "13         Hermione Granger       PERSON                 {be, along}   \n",
       "14             Harry Potter       PERSON              {as, disguise}   \n",
       "15         Hermione Granger       PERSON              {as, disguise}   \n",
       "16         Hermione Granger       PERSON                    {be, to}   \n",
       "17              Ron Weasley       PERSON              {as, disguise}   \n",
       "18           Thorfinn Rowle       PERSON  {be, punish, severely, by}   \n",
       "19           Thorfinn Rowle       PERSON           {also, fight, at}   \n",
       "20           Thorfinn Rowle       PERSON           {also, on, fight}   \n",
       "21             final defeat      Unknown         {be, uncertain, 's}   \n",
       "22              Rowle  fate      Unknown      {after, be, uncertain}   \n",
       "23                   defeat      Unknown      {after, be, uncertain}   \n",
       "24             final defeat      Unknown      {after, be, uncertain}   \n",
       "\n",
       "    relation_cluster_number                             object object_type  \n",
       "0                         4                               Dark     Unknown  \n",
       "1                         4                        Dark wizard     Unknown  \n",
       "2                         1           the Second Wizarding War       EVENT  \n",
       "3                         4                             wizard     Unknown  \n",
       "4                         5                       Harry Potter      PERSON  \n",
       "5                         0               Rubeus Hagrid  cabin     Unknown  \n",
       "6                         0          fellow Death Eater Gibbon     Unknown  \n",
       "7                         0                 Death Eater Gibbon     Unknown  \n",
       "8                         6          Battle of Astronomy Tower     Unknown  \n",
       "9                         6  the Battle of the Astronomy Tower       EVENT  \n",
       "10                        4                              cabin     Unknown  \n",
       "11                        0                   Hermione Granger      PERSON  \n",
       "12                        1                     1 August, 1997        DATE  \n",
       "13                        1                    Antonin Dolohov      PERSON  \n",
       "14                        3               construction workers     Unknown  \n",
       "15                        3               construction workers     Unknown  \n",
       "16                        4               Tottenham Court Road         FAC  \n",
       "17                        3               construction workers     Unknown  \n",
       "18                        5                     Lord Voldemort     Unknown  \n",
       "19                        6             the Battle of Hogwarts       EVENT  \n",
       "20                        6                         2 May 1998     Unknown  \n",
       "21                        7                          Voldemort      PERSON  \n",
       "22                        7           the Second Wizarding War       EVENT  \n",
       "23                        7           the Second Wizarding War       EVENT  \n",
       "24                        7           the Second Wizarding War       EVENT  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the processed dataframe\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar relation cluster in meaning:  ['also on fight', 'also have in fight', 'also have fight', 'also fight at', 'fight at']\n",
      "Number of the relation cluster:  6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>subject_type</th>\n",
       "      <th>relation</th>\n",
       "      <th>relation_cluster_number</th>\n",
       "      <th>object</th>\n",
       "      <th>object_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{also, fight, at}</td>\n",
       "      <td>6</td>\n",
       "      <td>the Battle of the Astronomy Tower</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{also, fight, at}</td>\n",
       "      <td>6</td>\n",
       "      <td>the Battle of Hogwarts</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Thorfinn Rowle</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>{also, have, in, fight}</td>\n",
       "      <td>6</td>\n",
       "      <td>the First Wizarding War</td>\n",
       "      <td>EVENT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           subject subject_type                 relation  \\\n",
       "9   Thorfinn Rowle       PERSON        {also, fight, at}   \n",
       "19  Thorfinn Rowle       PERSON        {also, fight, at}   \n",
       "31  Thorfinn Rowle       PERSON  {also, have, in, fight}   \n",
       "\n",
       "    relation_cluster_number                             object object_type  \n",
       "9                         6  the Battle of the Astronomy Tower       EVENT  \n",
       "19                        6             the Battle of Hogwarts       EVENT  \n",
       "31                        6            the First Wizarding War       EVENT  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you may filter now by relation. Choose any string to filter by. \n",
    "# The closest relation cluster in meaning will be chosen.\n",
    "# you may also filter by subject_type and object_type while filtering using the optional arguments\n",
    "filter_relations(df,vectors,d,  relation_string='fighting', subject_type='PERSON', object_type='EVENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all cells below first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    \"\"\"Open txt file from path and return read file contents\"\"\"\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    \n",
    "    with open(file_path, \"r\", encoding='utf-8') as file_handle:\n",
    "        file_contents = file_handle.read()\n",
    "        return file_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:44:32.190047Z",
     "start_time": "2021-10-27T11:44:32.178919Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_batch_corpus(input_text, max_caracter=5000):\n",
    "    import nltk\n",
    "    # \"input_text\" is a list with one entry which is a long string of the text to be translated\n",
    "    corpus = nltk.sent_tokenize(input_text)\n",
    "    \n",
    "    # Size information\n",
    "    nb_sentence = len(corpus)\n",
    "\n",
    "    # Batch information (reset these values after each batch finalization)\n",
    "    batch = []\n",
    "    batch_length = 0\n",
    "    \n",
    "\n",
    "    # All batches are stored in that list, which will bbe the output of the function\n",
    "    batch_corpus = []\n",
    "    \n",
    "    # Going throug each sentence of the initial corpus to create the batches\n",
    "    for idx, sentence in enumerate(corpus):\n",
    "        \n",
    "        # Are we dealing with the last sentence ?\n",
    "        last_sentence = idx + 1 == nb_sentence\n",
    "\n",
    "        # Checking the batch size before adding a new sentence in it\n",
    "        hypothetical_length = batch_length + len(sentence)\n",
    "        if hypothetical_length < max_caracter:\n",
    "            batch.append(sentence)\n",
    "            batch_length += len(sentence) # + len(joiner)\n",
    "            \n",
    "            # If sentence can be added to the corpus wa add it and don't save the corpus yet\n",
    "            # Except if this is the last sentence\n",
    "            if not last_sentence:\n",
    "                continue\n",
    "        \n",
    "        # Finalizing batch beforee storing\n",
    "        joined_batch = \"\".join(batch)\n",
    "    \n",
    "        # Save batch in the corpus\n",
    "        batch_corpus.append(joined_batch)\n",
    "        \n",
    "        # Reseting batch parameters\n",
    "        batch = []\n",
    "        batch_length = 0\n",
    "                    \n",
    "    return batch_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coreference resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:44:32.771797Z",
     "start_time": "2021-10-27T11:44:32.757579Z"
    }
   },
   "outputs": [],
   "source": [
    "def resolved_text_sentencizer(paragraphs):\n",
    "    \"\"\"Performs coreference resolution on a list of batches with raw input strings and returns a list of resolved batches\"\"\"\n",
    "    from tqdm.notebook import tqdm\n",
    "    import coreferee, spacy\n",
    "    import spacy_transformers\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "    nlp.add_pipe('coreferee')\n",
    "    end = len(paragraphs)\n",
    "    sentences = []\n",
    "    for i, text in tqdm(enumerate(paragraphs), total=end):\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            resolved_sentence = []\n",
    "            for i,token in enumerate(doc):\n",
    "                res = doc._.coref_chains.resolve(doc[i])\n",
    "                if type(res) == list:\n",
    "                    res = \" and \".join(r.text for r in res)\n",
    "                elif res == None:\n",
    "                    res = token.text\n",
    "#                 print((token.text, doc._.coref_chains.resolve(doc[i])))\n",
    "                resolved_sentence.append(res)\n",
    "            resolved_text = \" \".join(resolved_sentence)\n",
    "            sentences.append(resolved_text)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# output = [sent for sent in sentences if 'president' in \n",
    "#           (' '.join([token.lemma_.lower() for token in nlp(sent)]))]\n",
    "# print('Fact count:', len(output))\n",
    "# for fact in range(len(output)):\n",
    "#     print(str(fact+1)+'.', output[fact])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triples extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:44:34.065609Z",
     "start_time": "2021-10-27T11:44:34.058567Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def triple_extraction(sentences, num_sentences):\n",
    "    \"\"\"Extract subject-relation-object triples from the given input string and returns list of triple dictionaries (triple corpus)\"\"\"\n",
    "    print('extracting triples')\n",
    "#     import pandas as pd\n",
    "    from tqdm.notebook import tqdm\n",
    "    end = num_sentences\n",
    "    # print('Number of sentences: ', end)\n",
    "    data = []\n",
    "\n",
    "    from openie import StanfordOpenIE\n",
    "    # https://stanfordnlp.github.io/CoreNLP/openie.html\n",
    "# Default value of openie.affinity_probability_cap was 1/3.\n",
    "    properties = {\n",
    "#         'openie.affinity_probability_cap': 2 / 3\n",
    "#         'openie.splitter.disable' : True,\n",
    "        'openie.triple.strict' : False,\n",
    "        'openie.triple.all_nominals': True\n",
    "        \n",
    "    }\n",
    "\n",
    "    with StanfordOpenIE(properties=properties) as client:\n",
    "\n",
    "        for i, sent in tqdm(enumerate(sentences), total=end):\n",
    "            try:\n",
    "        #         print('Current sentence: ', sent)\n",
    "                extracted_triples = client.annotate(sent)\n",
    "#                 for triple in extracted_triples: #add the oriningal sentence to the extratced triples\n",
    "#                     triple['sentence'] = sent\n",
    "        #         print('Found %s triples in the sentence.' % len(extracted_triples))\n",
    "    #             for triple in extracted_triples:\n",
    "    #                 print('|-', triple)\n",
    "                data.extend(extracted_triples)\n",
    "            except:\n",
    "                pass\n",
    "    triples_corpus = pd.DataFrame(data)\n",
    "#     print('DataFrame appended. triples_corpus: ', triples_corpus)\n",
    "    return triples_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-27T11:44:34.683696Z",
     "start_time": "2021-10-27T11:44:34.673654Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_processor_pipeline(text):\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pickle\n",
    "    from datetime import timedelta\n",
    "    import time\n",
    "    \"\"\"Takes in a raw input string and performs, coreference resolution, pass2act and OpenIE triple extraction\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('preparing batch corpus...')\n",
    "    batch_corpus = prepare_batch_corpus(text)\n",
    "    end_time = time.time()\n",
    "    print('Batch corpus prepared. Number of paragraphs = ', len(batch_corpus))\n",
    "    print('Time taken for preperation: ', str(timedelta(seconds=end_time-start_time)))\n",
    "#     print(batch_corpus)\n",
    "    \n",
    "    with open(('batch_corpus' + '.pickle'), 'wb') as file:\n",
    "        pickle.dump(batch_corpus, file)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print('resolving sentences...')\n",
    "    resolved_batches = resolved_text_sentencizer(batch_corpus)\n",
    "    end_time = time.time()\n",
    "    print('sentences resolved. Number of sentences = ', len(resolved_batches))\n",
    "    print('Time taken for resolving: ', str(timedelta(seconds=end_time-start_time)))\n",
    "#     print(resolved_batches)\n",
    "    with open(('resolved_batches' + '.pickle'), 'wb') as file:\n",
    "        pickle.dump(resolved_batches, file)\n",
    "\n",
    "    active_sentences = resolved_batches\n",
    "    \n",
    "    start_time = time.time()\n",
    "    triples_list = []\n",
    "    end = len(active_sentences)\n",
    "    print('extracting triples')\n",
    "    triples_corpus = triple_extraction(active_sentences, end)\n",
    "    end_time = time.time()\n",
    "    print('Time taken for triple extraction: ', str(timedelta(seconds=end_time-start_time)))\n",
    "    \n",
    "    with open(('triples_corpus' + '.pickle'), 'wb') as file:\n",
    "        pickle.dump(triples_corpus, file)\n",
    "    \n",
    "    return triples_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relationship Extraction for Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_named_entity_set(raw_text): #TODO: sp√§ter mit fuzzy matchin erweitern / Nur bestimmte entity typen filtern\n",
    "    from thefuzz import fuzz\n",
    "    from thefuzz import process\n",
    "    doc = nlp(raw_text)\n",
    "    if doc.ents:\n",
    "        ent_list = [(ent.text, ent.label_) for ent in doc.ents if not ent.label_ in ['MONEY', 'ORDINAL', 'CARDINAL', 'QUANTITY', 'TIME', 'PERCENT']]\n",
    "    ent_set = set(ent_list)\n",
    "\n",
    "    NE_set = set()\n",
    "\n",
    "    for ent in ent_set:\n",
    "        if not ent in NE_set:\n",
    "            for ent2 in NE_set:\n",
    "                if ent2[0] in ent[0]: # An element in the set is part of the new ent\n",
    "                    NE_set.remove(ent2)\n",
    "                    NE_set.add(ent)\n",
    "                    break\n",
    "                elif ent[0] in ent2[0]: #the new element is part of an existing element in the set\n",
    "                    break\n",
    "            else: # we have not existed with either of the cases so it is a completely new ent\n",
    "                NE_set.add(ent)\n",
    "    return NE_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_relation_dict(NE_set, triple_df):\n",
    "    from collections import defaultdict\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    relation_dict = defaultdict(set)\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "    for index, row in tqdm(triple_df.iterrows(), total=triple_df.shape[0]):\n",
    "        # check if both subject and object are Named Entities by checking \n",
    "        # if they are part of the entries in the ent_set\n",
    "        sub = None\n",
    "        obj = None\n",
    "        \n",
    "        for ent in NE_set:\n",
    "            if row[\"subject\"] in ent[0]:\n",
    "                sub = ent\n",
    "                break # subject is part of an ent\n",
    "#             else: #sub or obj is NE\n",
    "#                 continue # subject is no NE\n",
    "        for ent in NE_set:\n",
    "            if row[\"object\"] in ent[0]:\n",
    "                obj = ent\n",
    "                break # object is part of ent\n",
    "#             else:\n",
    "#                 continue # Object is no NE\n",
    "        if sub or obj: #one of them is NEs\n",
    "            relation_doc = nlp(row[\"relation\"])\n",
    "            relation_lemmas = [token.lemma_ for token in relation_doc ] #if token.lemma_ not in nlp.Defaults.stop_words and token.pos_ in [\"ADJ\", \"ADP\", \"ADV\", \"AUX\", \"VERB\"]]\n",
    "#                 print(row[\"subject\"])\n",
    "#                 print(row[\"object\"])\n",
    "#                 print(relation_lemmas)\n",
    "            if not sub:\n",
    "                sub = row[\"subject\"]\n",
    "            elif not obj:\n",
    "                obj = row[\"object\"]\n",
    "            if sub == obj: #cycle relation\n",
    "                continue\n",
    "            relation_dict[(sub, obj)].update(relation_lemmas)\n",
    "\n",
    "    return relation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(relation_df):\n",
    "    import networkx as nx\n",
    "    g = nx.from_pandas_edgelist(relation_df,\n",
    "                                'subject',\n",
    "                                'object',\n",
    "                                 edge_attr='relation',\n",
    "                                 create_using=nx.DiGraph(),) \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_dict_to_pandas_edge_list(relation_dict):\n",
    "#     import pandas as pd\n",
    "    columns=['subject','relation','object']\n",
    "    data = []\n",
    "    from tqdm.notebook import tqdm\n",
    "    for item in tqdm(relation_dict.items(), total=len(relation_dict)):\n",
    "#     for item in relation_dict.items():\n",
    "        \n",
    "        values = [item[0][0],item[1],item[0][1]]\n",
    "        zipped = zip(columns, values)\n",
    "        a_dictionary = dict(zipped)\n",
    "#         print(a_dictionary)\n",
    "        data.append(a_dictionary)\n",
    "    df = pd.DataFrame(columns=['subject','relation','object'])\n",
    "    df = df.append(data, True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(df, name):\n",
    "    df.to_excel(name + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(input_df, raw_text, distance_treshold=15):\n",
    "    NE_set = create_named_entity_set(raw_text)\n",
    "    relation_dict = create_graph_relation_dict(NE_set, input_df)\n",
    "    df = relation_dict_to_pandas_edge_list(relation_dict)\n",
    "    df = clean_df(df)\n",
    "    relation_for_clustering = df['relation'].tolist()\n",
    "    relation_list = [' '.join(element) if type(element) is set else element for element in relation_for_clustering]\n",
    "    relation_list_set = set(relation_list)\n",
    "    relation_list_set = list(relation_list_set)\n",
    "    linkage, embeddings_sentences = hierarchical_relation_clustering(relation_list_set)\n",
    "    cluster = clustering(linkage, embeddings_sentences, distance_treshold)\n",
    "    dictionary_relation_to_cluster = dict(zip(relation_list_set, cluster.labels_))\n",
    "    cluster_association_list = [dictionary_relation_to_cluster[element] for element in relation_list]\n",
    "    df['relation_cluster_number'] = cluster_association_list\n",
    "    cols = ['subject','subject_type', 'relation', 'relation_cluster_number', 'object', 'object_type']\n",
    "    df = df[cols]\n",
    "    return df, cluster, relation_list_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_relation_clustering(relation_list_set):\n",
    "    sentences = relation_list_set\n",
    "    embeddings_sentences = model.encode(sentences)\n",
    "    l = shc.linkage(embeddings_sentences, method='ward')\n",
    "    return l, embeddings_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(linkage, embeddings_sentences, distance_treshold=15):\n",
    "    # Initialize hiererchial clustering method, in order for the algorithm to determine the number of clusters\n",
    "    cluster = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='ward', compute_full_tree=True, distance_threshold=distance_treshold)\n",
    "\n",
    "    # Cluster the data\n",
    "    cluster.fit_predict(embeddings_sentences)\n",
    "    print(f\"Number of clusters = {1+np.amax(cluster.labels_)}\")\n",
    "\n",
    "    print(\"Classifying the points into clusters:\")\n",
    "    print(cluster.labels_)\n",
    "\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    subjects = df['subject'].tolist()\n",
    "    # [f(x) if condition else g(x) for x in sequence]\n",
    "    subject_list = [element[0] if type(element) is tuple else element for element in subjects]\n",
    "    subject_type_list = [element[1] if type(element) is tuple else 'Unknown' for element in subjects]\n",
    "    subject_list_cleaned = [element.replace(\"'s\", \"\").strip() for element in subject_list]\n",
    "    df['subject'] = subject_list_cleaned\n",
    "    df['subject_type'] = subject_type_list\n",
    "    objects = df['object'].tolist()\n",
    "    # [f(x) if condition else g(x) for x in sequence]\n",
    "    object_list = [element[0] if type(element) is tuple else element for element in objects]\n",
    "    object_type_list = [element[1] if type(element) is tuple else 'Unknown' for element in objects]\n",
    "    object_list_cleaned = [element.replace(\"'s\", \"\").strip() for element in object_list]\n",
    "    df['object'] = object_list_cleaned\n",
    "    df['object_type'] = object_type_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_filtering(cluster, relation_list_set):\n",
    "    sentences = relation_list_set\n",
    "    d = defaultdict(list)\n",
    "    for i, entry in enumerate(cluster.labels_):\n",
    "        d[entry].append(sentences[i])\n",
    "    \n",
    "    avg_vec = defaultdict(list)\n",
    "    vectors = []\n",
    "    for key, value in tqdm(sorted(d.items()), total=len(d)):\n",
    "    #     print(key)\n",
    "        embeddings_sentences = model.encode(value)\n",
    "        embeddings = np.mean(np.array(embeddings_sentences), axis=0)\n",
    "        avg_vec[key].append(embeddings)\n",
    "        vectors.append(embeddings)\n",
    "    return vectors, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relations(df, vectors, d, relation_string, subject_type=None, object_type=None):\n",
    "    new_word = model.encode(['fighting'])\n",
    "    scores = cosine_similarity(new_word, vectors).flatten()\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_score_index = 0\n",
    "    for i, score in enumerate(scores):\n",
    "        if highest_score < score:\n",
    "            highest_score = score\n",
    "            highest_score_index = i\n",
    "\n",
    "    most_similar_document = d[highest_score_index]\n",
    "    print('Most similar relation cluster in meaning: ', most_similar_document)\n",
    "    print('Number of the relation cluster: ', highest_score_index)\n",
    "    filtered_df = df\n",
    "    if subject_type:\n",
    "        filtered_df = filtered_df[filtered_df['subject_type']==subject_type]\n",
    "    if object_type:\n",
    "        filtered_df = filtered_df[filtered_df['object_type']==object_type]\n",
    "    filtered_df = filtered_df[filtered_df['relation_cluster_number']== highest_score_index]\n",
    "\n",
    "    return filtered_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
