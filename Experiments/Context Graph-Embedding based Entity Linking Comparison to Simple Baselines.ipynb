{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9474db3",
   "metadata": {},
   "source": [
    "# Comparison to Baseline Systems\n",
    "This notebooks execution has not been tested and is meant for viewing.\n",
    "\n",
    "In this notebook the comparison of the adapted system design which uses context graph-embeddings for entity linking to simple baseline approaches is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bad74f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f7d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samar Syed\\anaconda3\\envs\\doc2db\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from fuzzywuzzy import process\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca6e1294",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fb4e83d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x18d66f6d308>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spacy Pipeline Components\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0077f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[My sister: [My sister, She], a dog: [a dog, him]]\n",
      "Angela: [Angela, She]\n",
      "Boston: [Boston, that city]\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('My sister has a dog. She loves him.')\n",
    "print(doc1._.coref_clusters)\n",
    "\n",
    "doc2 = nlp('Angela lives in Boston. She is quite happy in that city.')\n",
    "for ent in doc2.ents:\n",
    "    print(ent._.coref_cluster)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200de471",
   "metadata": {},
   "source": [
    "# spaCy Stanza\n",
    "For spaCy version 2.x \n",
    "More info\n",
    "https://github.com/explosion/spacy-stanza/tree/2855138759541c149e277c97b892a0a1889bb609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ec85b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-23 09:45:21 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-03-23 09:45:21 INFO: Use device: cpu\n",
      "2022-03-23 09:45:21 INFO: Loading: tokenize\n",
      "2022-03-23 09:45:22 INFO: Loading: pos\n",
      "2022-03-23 09:45:22 INFO: Loading: lemma\n",
      "2022-03-23 09:45:22 INFO: Loading: depparse\n",
      "2022-03-23 09:45:22 INFO: Loading: sentiment\n",
      "2022-03-23 09:45:23 INFO: Loading: ner\n",
      "2022-03-23 09:45:24 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Barack PROPN nsubj:pass PERSON\n",
      "Obama Obama PROPN flat PERSON\n",
      "was be AUX aux:pass \n",
      "born bear VERB root \n",
      "in in ADP case \n",
      "Hawaii Hawaii PROPN obl GPE\n",
      ". . PUNCT punct \n",
      "He he PRON nsubj:pass \n",
      "was be AUX aux:pass \n",
      "elected elect VERB root \n",
      "president president NOUN xcomp \n",
      "in in ADP case \n",
      "2008 2008 NUM obl DATE\n",
      ". . PUNCT punct \n",
      "(Barack Obama, Hawaii, 2008)\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "from spacy_stanza import StanzaLanguage\n",
    "\n",
    "snlp = stanza.Pipeline(lang=\"en\")\n",
    "nlp = StanzaLanguage(snlp)\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii. He was elected president in 2008.\")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2915d73a",
   "metadata": {},
   "source": [
    "# Matching Documents using Document Similarity Measures\n",
    "We will use Document Similarity Measures to compute the Similartiy between documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4854306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikibase Item QID</th>\n",
       "      <th>Article Name</th>\n",
       "      <th>Wikipedia Article Link</th>\n",
       "      <th>Article Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q445486</td>\n",
       "      <td>Altstadt (Düsseldorf)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Altstadt_(D%C3%B...</td>\n",
       "      <td>The Altstadt (literally \"old town\") is one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q449266</td>\n",
       "      <td>Altstadt (Frankfurt am Main)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Altstadt_(Frankf...</td>\n",
       "      <td>The Altstadt (old town) is a quarter (Stadttei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q819081</td>\n",
       "      <td>Topography of Terror</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Topography_of_Te...</td>\n",
       "      <td>The Topography of Terror (German: Topographie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1973070</td>\n",
       "      <td>Neanderkirche</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Neanderkirche</td>\n",
       "      <td>The Neanderkirche (Neander Church) is a Protes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q679491</td>\n",
       "      <td>Rheinturm</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rheinturm</td>\n",
       "      <td>The Rheinturm (pronounced [ˈʁaɪ̯ntʊʁm]) (Rhine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Wikibase Item QID                  Article Name  \\\n",
       "0           Q445486         Altstadt (Düsseldorf)   \n",
       "1           Q449266  Altstadt (Frankfurt am Main)   \n",
       "2           Q819081          Topography of Terror   \n",
       "3          Q1973070                 Neanderkirche   \n",
       "4           Q679491                     Rheinturm   \n",
       "\n",
       "                              Wikipedia Article Link  \\\n",
       "0  https://en.wikipedia.org/wiki/Altstadt_(D%C3%B...   \n",
       "1  https://en.wikipedia.org/wiki/Altstadt_(Frankf...   \n",
       "2  https://en.wikipedia.org/wiki/Topography_of_Te...   \n",
       "3        https://en.wikipedia.org/wiki/Neanderkirche   \n",
       "4            https://en.wikipedia.org/wiki/Rheinturm   \n",
       "\n",
       "                                     Article Summary  \n",
       "0  The Altstadt (literally \"old town\") is one of ...  \n",
       "1  The Altstadt (old town) is a quarter (Stadttei...  \n",
       "2  The Topography of Terror (German: Topographie ...  \n",
       "3  The Neanderkirche (Neander Church) is a Protes...  \n",
       "4  The Rheinturm (pronounced [ˈʁaɪ̯ntʊʁm]) (Rhine...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Excel will serve as our Ontology\n",
    "wikitravelLocationArticles = pd.read_excel('wikitravelLocationWikipediaArticles.xlsx')\n",
    "wikitravelLocationArticles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20428901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikitravel Article Link</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Description</th>\n",
       "      <th>Wikibase Item QID</th>\n",
       "      <th>Unique Link?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://wikitravel.org/en/D%C3%BCsseldorf</td>\n",
       "      <td>Altstadt</td>\n",
       "      <td>Old town (Altstadt), (U-Bahn stop: Heinrich-He...</td>\n",
       "      <td>Q445486</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://wikitravel.org/en/D%C3%BCsseldorf</td>\n",
       "      <td>Neanderkirche</td>\n",
       "      <td>The Neander-church (Neanderkirche) has its own...</td>\n",
       "      <td>Q1973070</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://wikitravel.org/en/D%C3%BCsseldorf</td>\n",
       "      <td>Rheinturm</td>\n",
       "      <td>Rhine Tower (Rheinturm) , (Tram stop: Platz de...</td>\n",
       "      <td>Q679491</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://wikitravel.org/en/London</td>\n",
       "      <td>Buckingham Palace</td>\n",
       "      <td>Buckingham Palace - The official London reside...</td>\n",
       "      <td>Q42182</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://wikitravel.org/en/London</td>\n",
       "      <td>Marble Arch</td>\n",
       "      <td>Marble Arch is a white Carrara marble monument...</td>\n",
       "      <td>Q845529</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Wikitravel Article Link            Mention  \\\n",
       "0  https://wikitravel.org/en/D%C3%BCsseldorf           Altstadt   \n",
       "1  https://wikitravel.org/en/D%C3%BCsseldorf      Neanderkirche   \n",
       "2  https://wikitravel.org/en/D%C3%BCsseldorf          Rheinturm   \n",
       "3           https://wikitravel.org/en/London  Buckingham Palace   \n",
       "4           https://wikitravel.org/en/London        Marble Arch   \n",
       "\n",
       "                                         Description Wikibase Item QID  \\\n",
       "0  Old town (Altstadt), (U-Bahn stop: Heinrich-He...           Q445486   \n",
       "1  The Neander-church (Neanderkirche) has its own...          Q1973070   \n",
       "2  Rhine Tower (Rheinturm) , (Tram stop: Platz de...           Q679491   \n",
       "3  Buckingham Palace - The official London reside...            Q42182   \n",
       "4  Marble Arch is a white Carrara marble monument...           Q845529   \n",
       "\n",
       "  Unique Link?  \n",
       "0            N  \n",
       "1            Y  \n",
       "2            Y  \n",
       "3            Y  \n",
       "4            Y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking = pd.read_excel('EntityLinkingDataset.xlsx')\n",
    "wikitravelEntityLinking.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d314644",
   "metadata": {},
   "source": [
    "# Candidate Set Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6bba349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple Inc.', 100), ('apple incorporated', 90), ('apple iphone', 76), ('App Incorp', 74), ('apply', 72), ('apps', 68), ('apple park', 67), ('apple south', 67), ('app store', 44), ('iphone', 30)]\n",
      "('Apple Inc.', 100)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Candidate Generation using FuzzyWuzzy String Matching\n",
    "from fuzzywuzzy import process\n",
    "str2Match = \"apple inc\"\n",
    "strOptions = [\"Apple Inc.\",\"apple park\",\"apple incorporated\",\"iphone\", \"apple south\", \"app store\", \"apple iphone\", \"apply\", \"apps\", \"App Incorp\"]\n",
    "Ratios = process.extract(str2Match,strOptions, limit=10)\n",
    "print(Ratios)\n",
    "truth = \"Apple Inc.\"\n",
    "truth_position = 100\n",
    "for i, option in enumerate(Ratios):\n",
    "    if(option[0] == truth):\n",
    "        truth_position = i\n",
    "        break\n",
    "# You can also select the string with the highest matching percentage\n",
    "highest = process.extractOne(str2Match,strOptions)\n",
    "print(highest)\n",
    "print(truth_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "75ac227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chinese garden', 100), ('Chinese Garden, Singapore', 90), ('Orangery (Royal Garden of Prague Castle)', 86), ('Royal Botanic Garden, Sydney', 86), ('Botanic Garden Zuidas', 86), ('Englischer Garten', 58), ('Grand Arche', 53), ('Chinatown, Darwin', 52), ('Glasgow Botanic Gardens', 51), ('Hessisches Landesmuseum Darmstadt', 51), ('Rheinisches Landesmuseum', 51), ('Rheinisches Landesmuseum Bonn', 51), ('Rheinisches Landesmuseum Trier', 51)]\n",
      "('Chinese garden', 100)\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# We will take the first Entry in the table as a Example\n",
    "mention = wikitravelEntityLinking['Mention'][9]\n",
    "# mention = 'Altstadt'\n",
    "\n",
    "str2Match = mention\n",
    "strOptions = wikitravelLocationArticles['Article Name'].tolist()\n",
    "ratios = process.extract(str2Match,strOptions, limit=None)\n",
    "ratios = [x for x in ratios if x[1]>50]\n",
    "print(ratios)\n",
    "\n",
    "# You can also select the string with the highest matching percentage\n",
    "highest = process.extractOne(str2Match,strOptions)\n",
    "print(highest)\n",
    "\n",
    "print(len(ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e7b3e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidate set generation (no limits / not on number of entries or lowest similarity score)\n",
    "wikitravelEntityLinking['Candidate Index Set'] = \"\"\n",
    "ontology = wikitravelLocationArticles['Article Name'].tolist()\n",
    "\n",
    "for index, row in wikitravelEntityLinking.iterrows():\n",
    "    index_set = []\n",
    "    mention = row['Mention']\n",
    "\n",
    "    ratios = process.extract(mention, ontology, limit=None)\n",
    "    ratios = [x for x in ratios if x[1]>50]\n",
    "    for candidate in ratios:\n",
    "        index_set.extend(wikitravelLocationArticles.index[wikitravelLocationArticles['Article Name'] == candidate[0]].tolist())\n",
    "    row['Candidate Index Set'] = index_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9ef1b",
   "metadata": {},
   "source": [
    "# Simple approach\n",
    "The metric for according to nlp-progress is micro-precision: Fraction of correctly disambiguated named entities in the full corpus.\n",
    "\n",
    "In the first naive approach we will first only assign the best fuzzy matching candidate from the generated candidate set.\n",
    "In further steps we will multiply the string matching values with further context similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "907f1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1973070'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelLocationArticles.loc[wikitravelLocationArticles['Article Name'] == 'Neanderkirche']['Wikibase Item QID'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1379866c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.761607\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "strOptions = wikitravelLocationArticles['Article Name'].tolist()\n",
    "wikitravelEntityLinking['Simple Prediction'] = \"\"\n",
    "predictions = []\n",
    "top_10_pred = []\n",
    "for index, row in wikitravelEntityLinking.iterrows():\n",
    "    # We will take the first Entry in the table as a Example\n",
    "    mention = row['Mention']\n",
    "    # mention = 'Altstadt'\n",
    "\n",
    "    str2Match = mention\n",
    "    ratios = process.extract(mention,strOptions, limit=10)\n",
    "\n",
    "    truth = row[\"Wikibase Item QID\"]\n",
    "    truth_position = 100\n",
    "    for i, option in enumerate(ratios):\n",
    "        pre_qid = wikitravelLocationArticles.loc[wikitravelLocationArticles['Article Name'] == option[0]]['Wikibase Item QID'].tolist()[0]\n",
    "        if(pre_qid == truth):\n",
    "            truth_position = i\n",
    "            break\n",
    "    top_10_pred.append(truth_position)\n",
    "\n",
    "    highest = process.extractOne(mention,strOptions)\n",
    "    pred = wikitravelLocationArticles.loc[wikitravelLocationArticles['Article Name'] == highest[0]]['Wikibase Item QID'].tolist()[0]\n",
    "    predictions.append(pred)\n",
    "\n",
    "wikitravelEntityLinking['Simple Prediction'] = predictions\n",
    "wikitravelEntityLinking['Simple Top 10 Prec'] = top_10_pred\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "19bf9a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results_simple_prediction(x):    \n",
    "    return True if x['Wikibase Item QID'] == x[\"Simple Prediction\"] else False\n",
    "wikitravelEntityLinking['Simple Predition Result'] = wikitravelEntityLinking.apply(check_results_simple_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "976c8203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     64\n",
       "False    16\n",
       "Name: Simple Predition Result, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking['Simple Predition Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e5503f94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     52\n",
       "False     2\n",
       "Name: Simple Predition Result, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'Y']['Simple Predition Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1688010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     12\n",
       "False     8\n",
       "Name: Simple Predition Result, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'N']['Simple Predition Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe06680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6\n",
       "Name: Simple Predition Result, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'NIL']['Simple Predition Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5198706",
   "metadata": {},
   "source": [
    "# Jaccard Context Comparison\n",
    "In the simple approach we saw that there are predictions that do not match even if the fuzzy mention-title matching gives a value of 90 - 100% matching rate. To ensure that the selcted candidate really is the right one we need to include the context of both the mention and the candidate and ensure that those are matching as well. If it is not the case we need to adapt and lower the ranking of that specific candidate and look at the context comparison with the next one.\n",
    "\n",
    "One very simple and basic paragraph or document comparision algorithm is the Jaccard Comparison which calculates the intersection of the word in two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "06e63011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 94, 90, 12, 21]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "base_document = wikitravelEntityLinking['Description'][0] # Altstadt Düsseldorf\n",
    "documents = wikitravelLocationArticles.iloc[wikitravelEntityLinking['Candidate Index Set'][0]]['Article Summary'].tolist()\n",
    "\n",
    "def preprocess(text):\n",
    "    # Steps:\n",
    "    # 1. lowercase\n",
    "    # 2. Lammetize. (It does not stem. Try to preserve structure not to overwrap with potential acronym).\n",
    "    # 3. Remove stop words.\n",
    "    # 4. Remove punctuations.\n",
    "    # 5. Remove character with the length size of 1.\n",
    "\n",
    "    lowered = str.lower(text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(lowered)\n",
    "\n",
    "    words = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            if w not in string.punctuation:\n",
    "                if len(w) > 1:\n",
    "                    lemmatized = lemmatizer.lemmatize(w)\n",
    "                    words.append(lemmatized)\n",
    "\n",
    "    return words\n",
    "\n",
    "def calculate_jaccard(word_tokens1, word_tokens2):\n",
    "    # Combine both tokens to find union.\n",
    "    both_tokens = word_tokens1 + word_tokens2\n",
    "    union = set(both_tokens)\n",
    "\n",
    "    # Calculate intersection.\n",
    "    intersection = set()\n",
    "    for w in word_tokens1:\n",
    "        if w in word_tokens2:\n",
    "            intersection.add(w)\n",
    "\n",
    "    jaccard_score = len(intersection)/len(union)\n",
    "    return jaccard_score\n",
    "\n",
    "def process_jaccard_similarity(base_document, documents):\n",
    "\n",
    "    # Tokenize the base document we are comparing against.\n",
    "    base_tokens = preprocess(base_document)\n",
    "\n",
    "    # Tokenize each document\n",
    "    all_tokens = []\n",
    "    for i, document in enumerate(documents):\n",
    "        tokens = preprocess(document)\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "    # print(\"making word tokens at index:\", i)\n",
    "\n",
    "    all_scores = []\n",
    "    for tokens in all_tokens:\n",
    "        score = calculate_jaccard(base_tokens, tokens)\n",
    "\n",
    "        all_scores.append(score)\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_score_index = 0\n",
    "    \n",
    "    sorted_index = sorted(range(len(all_scores)), key=lambda k: all_scores[k], reverse=True)\n",
    "    \n",
    "    highest_score_index = sorted_index[0]\n",
    "    highest_score = all_scores[sorted_index[0]]\n",
    "    \n",
    "#     for i, score in enumerate(all_scores):\n",
    "#         if highest_score < score:\n",
    "#             highest_score = score\n",
    "#             highest_score_index = i\n",
    "\n",
    "    most_similar_document_idxs = []\n",
    "    for idx in sorted_index:\n",
    "        current_doc = documents[idx]\n",
    "        most_similar_document_index = wikitravelLocationArticles.index[wikitravelLocationArticles['Article Summary'] == current_doc].tolist()[0]\n",
    "        most_similar_document_idxs.append(most_similar_document_index)\n",
    "        \n",
    "    most_similar_document = documents[highest_score_index]\n",
    "\n",
    "    # print(\"Most similar document by Jaccard with the score:\", most_similar_document, highest_score)\n",
    "    most_similar_document_index = wikitravelLocationArticles.index[wikitravelLocationArticles['Article Summary'] == most_similar_document].tolist()[0]\n",
    "    # print('Index of most similar document ', most_similar_document_index)\n",
    "#     if highest_score > 0.02: \n",
    "    return most_similar_document_idxs\n",
    "#     else:\n",
    "#         return \"NIL\"\n",
    "\n",
    "start = timer()\n",
    "process_jaccard_similarity(base_document, documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352d35f",
   "metadata": {},
   "source": [
    "\"NIL\" Prediction cannot be done on this base since the jaccard comparision values are very low and a good threshold cannot be generally defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35eec2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wikitravelLocationArticles['Wikibase Item QID'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7bacca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:02.722944\n"
     ]
    }
   ],
   "source": [
    "wikitravelEntityLinking['Jaccard Context Prediction'] = \"\"\n",
    "jaccard_context_prediction = []\n",
    "top_10_pred = []\n",
    "for index, row in wikitravelEntityLinking.iterrows():\n",
    "    \n",
    "    base_document = row['Description']\n",
    "    documents = wikitravelLocationArticles.iloc[row['Candidate Index Set']]['Article Summary'].tolist()\n",
    "\n",
    "    jaccard_pred = process_jaccard_similarity(base_document, documents)\n",
    "#     print('jaccard_pred = ', jaccard_pred)\n",
    "#     if jaccard_pred == \"NIL\":\n",
    "#         jaccard_context_prediction.append(jaccard_pred)\n",
    "#     else:\n",
    "    truth_position = 100\n",
    "    truth = row[\"Wikibase Item QID\"]\n",
    "    for i, option in enumerate(jaccard_pred):\n",
    "        pre_qid = wikitravelLocationArticles['Wikibase Item QID'][option]\n",
    "        if(pre_qid == truth):\n",
    "            truth_position = i\n",
    "            break\n",
    "    top_10_pred.append(truth_position)\n",
    "    \n",
    "\n",
    "    pred = wikitravelLocationArticles['Wikibase Item QID'][jaccard_pred[0]]\n",
    "        \n",
    "        \n",
    "#     print('pred = ', pred)\n",
    "    jaccard_context_prediction.append(pred)\n",
    "wikitravelEntityLinking['Jaccard Top 10 Prec'] = top_10_pred\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b13aa2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitravelEntityLinking['Jaccard Context Prediction'] = jaccard_context_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2ba4dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results_jaccard_context_prediction(x):    \n",
    "    return True if x['Wikibase Item QID'] == x[\"Jaccard Context Prediction\"] else False\n",
    "wikitravelEntityLinking['Jaccard Context Prediction Result'] = wikitravelEntityLinking.apply(check_results_jaccard_context_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bcd7713a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     59\n",
       "False    21\n",
       "Name: Jaccard Context Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking['Jaccard Context Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c40523aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     47\n",
       "False     7\n",
       "Name: Jaccard Context Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'Y']['Jaccard Context Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d7afac46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     12\n",
       "False     8\n",
       "Name: Jaccard Context Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'N']['Jaccard Context Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "055d9185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6\n",
       "Name: Jaccard Context Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'NIL']['Jaccard Context Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d9b14",
   "metadata": {},
   "source": [
    "# BERT Document Similarity\n",
    "Now we will use approved state-of-the-art Document embeddings using sentence bert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a440a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "base_document = wikitravelEntityLinking['Description'][2] # Altstadt Düsseldorf\n",
    "documents = wikitravelLocationArticles.iloc[wikitravelEntityLinking['Candidate Index Set'][2]]['Article Summary'].tolist()\n",
    "\n",
    "def process_bert_similarity(base_document, documents):\n",
    "    # This will download and load the pretrained model offered by UKPLab.\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "    # Although it is not explicitly stated in the official document of sentence transformer, the original BERT is meant for a shorter sentence. We will feed the model by sentences instead of the whole documents.\n",
    "    sentences = sent_tokenize(base_document)\n",
    "    base_embeddings_sentences = model.encode(sentences)\n",
    "    base_embeddings = np.mean(np.array(base_embeddings_sentences), axis=0)\n",
    "\n",
    "    vectors = []\n",
    "    for i, document in enumerate(documents):\n",
    "\n",
    "        sentences = sent_tokenize(document)\n",
    "        embeddings_sentences = model.encode(sentences)\n",
    "        embeddings = np.mean(np.array(embeddings_sentences), axis=0)\n",
    "        \n",
    "        vectors.append(embeddings)\n",
    "\n",
    "#         print(\"making vector at index:\", i)\n",
    "        \n",
    "    scores = cosine_similarity([base_embeddings], vectors).flatten()\n",
    "\n",
    "    highest_score = 0\n",
    "    highest_score_index = 0\n",
    "    for i, score in enumerate(scores):\n",
    "        if highest_score < score:\n",
    "            highest_score = score\n",
    "            highest_score_index = i\n",
    "\n",
    "    most_similar_document = documents[highest_score_index]\n",
    "#     print(\"Most similar document by BERT with the score:\", most_similar_document, highest_score)\n",
    "    most_similar_document_index = wikitravelLocationArticles.index[wikitravelLocationArticles['Article Summary'] == most_similar_document].tolist()[0]\n",
    "    # print('Index of most similar document ', most_similar_document_index)\n",
    "#     if highest_score > 0.02: \n",
    "    return most_similar_document_index\n",
    "#     else:\n",
    "#         return \"NIL\"\n",
    "\n",
    "start = timer()\n",
    "process_bert_similarity(base_document, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "165e9216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:15:25.868161\n"
     ]
    }
   ],
   "source": [
    "wikitravelEntityLinking['BERT Prediction'] = \"\"\n",
    "BERT_prediction = []\n",
    "for index, row in wikitravelEntityLinking.iterrows():\n",
    "    \n",
    "    base_document = row['Description']\n",
    "    documents = wikitravelLocationArticles.iloc[row['Candidate Index Set']]['Article Summary'].tolist()\n",
    "\n",
    "    BERT_pred = process_bert_similarity(base_document, documents)\n",
    "#     print('jaccard_pred = ', jaccard_pred)\n",
    "#     if jaccard_pred == \"NIL\":\n",
    "#         jaccard_context_prediction.append(jaccard_pred)\n",
    "#     else:\n",
    "    pred = wikitravelLocationArticles['Wikibase Item QID'][BERT_pred]\n",
    "#     print('pred = ', pred)\n",
    "    BERT_prediction.append(pred)\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c16396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitravelEntityLinking['BERT Prediction'] = BERT_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60412a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results_BERT_prediction(x):    \n",
    "    return True if x['Wikibase Item QID'] == x[\"BERT Prediction\"] else False\n",
    "wikitravelEntityLinking['BERT Prediction Result'] = wikitravelEntityLinking.apply(check_results_BERT_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85a844cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     50\n",
       "False    30\n",
       "Name: BERT Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking['BERT Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "019c06e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     41\n",
       "False    13\n",
       "Name: BERT Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'Y']['BERT Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "515566a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    11\n",
       "True      9\n",
       "Name: BERT Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'N']['BERT Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4bc42a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6\n",
       "Name: BERT Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'NIL']['BERT Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d80a0",
   "metadata": {},
   "source": [
    "# Context Graph Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd7fa012",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitravelEntityLinking['Context Graph Embedding Prediction'] = \"\"\n",
    "context_graph_embedding_prediction = []\n",
    "for index, row in wikitravelEntityLinking.iterrows():\n",
    "    \n",
    "    base_document = row['Description']\n",
    "    documents = wikitravelLocationArticles.iloc[row['Candidate Index Set']]['Article Summary'].tolist()\n",
    "    \n",
    "    context_graph_embedding_prediction_pred = context_graph_embedding(base_document, documents)\n",
    "    \n",
    "    if context_graph_embedding_prediction_pred == \"NIL\":\n",
    "        context_graph_embedding_prediction_prediction.append(context_graph_embedding_prediction_pred)\n",
    "    else:\n",
    "        pred = wikitravelLocationArticles['Wikibase Item QID'][context_graph_embedding_prediction_pred]\n",
    "        context_graph_embedding_prediction_prediction.append(pred)\n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "161673ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitravelEntityLinking['context_graph_embedding_prediction Prediction'] = context_graph_embedding_prediction_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0349c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results_context_graph_embedding_prediction_prediction(x):    \n",
    "    return True if x['Wikibase Item QID'] == x[\"context_graph_embedding_prediction Prediction\"] else False\n",
    "wikitravelEntityLinking['context_graph_embedding_prediction Prediction Result'] = wikitravelEntityLinking.apply(check_results_context_graph_embedding_prediction_prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ffa840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     52\n",
       "False    28\n",
       "Name: context_graph_embedding_prediction Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking['context_graph_embedding_prediction Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40abc8ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     41\n",
       "False    13\n",
       "Name: context_graph_embedding_prediction Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'Y']['context_graph_embedding_prediction Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce79eaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     11\n",
       "False     9\n",
       "Name: context_graph_embedding_prediction Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'N']['context_graph_embedding_prediction Prediction Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dca5ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    6\n",
       "Name: context_graph_embedding_prediction Prediction Result, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitravelEntityLinking.loc[wikitravelEntityLinking['Unique Link?'] == 'NIL']['context_graph_embedding_prediction Prediction Result'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
